<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Machine Learning with tidymodels</title>
    <meta charset="utf-8" />
    <meta name="author" content="Uli Niemann" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
    <link rel="stylesheet" href="assets\css\my-theme.css" type="text/css" />
    <link rel="stylesheet" href="assets\css\my-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: title-slide, center, bottom

# 05 - Machine Learning with tidymodels

## Data Science with R &amp;#183; Summer 2020

### Uli Niemann &amp;#183; Knowledge Management &amp; Discovery Lab

#### [https://brain.cs.uni-magdeburg.de/kmd/DataSciR/](https://brain.cs.uni-magdeburg.de/kmd/DataSciR/)

.courtesy[&amp;#x1F4F7; Photo courtesy of Ulrich Arendt]

---

## tidymodels

&lt;img src="figures/05-tidymodels//tidymodels-workflow.png" width="100%" /&gt;

&lt;!-- ## tidymodels ecosystem --&gt;

???

tidymodels is a "meta-package" for modeling and statistical analysis that share the underlying design philosophy, grammar, and data structures of the tidyverse.

Today, we will cover the packages

- parsnip: tidy, unified interface to creating models
- rsample: resampling data
- yardstick: model evaluation (metrics such as accuracy, RMSE)
- tune: hyperparameter optimization
- workflows: combine pre-processing steps and models into single objects
- recipes: data preprocessing: feature engineering, imputation, etc
- dials? has tools to create and manage values of tuning parameters.

---

class: bottom, center

background-image: url("figures/05-tidymodels/caret-obs.png")
background-size: contain

&lt;!-- .font100[Data Science with R WS 2018/19] --&gt;

---

## Robust and capable alternative: [mlr3](https://mlr3.mlr-org.com/)

&lt;img src="https://raw.githubusercontent.com/mlr-org/mlr3/master/man/figures/mlr3verse.svg?sanitize=true" width="70%" style="display: block; margin: auto;" /&gt;

.footnote[Figure source: &lt;https://mlr3.mlr-org.com/&gt;]

???

- Currently provides more comprehensive functionalities than tidymodels
- tidymodels is likely to catch up until 2021

---

class: middle

This tutorial is a condensed version of the 2-day
workshop ["Introduction to Machine Learning with the Tidyverse"](https://conf20-intro-ml.netlify.app/) 
held by Dr. Alison Hill at the 
[rstudio::conf 2020](https://rstudio.com/conference/).

&lt;iframe src="https://conf20-intro-ml.netlify.app/" width="100%" height="450px"&gt;&lt;/iframe&gt;

---

## Setup


```r
library(tidyverse)
```

```
## -- Attaching packages ------------------------------------------------------ tidyverse 1.3.0 --
```

```
## v ggplot2 3.3.0.9000     v purrr   0.3.3     
## v tibble  3.0.0          v dplyr   0.8.5     
## v tidyr   1.0.2          v stringr 1.4.0     
## v readr   1.3.1          v forcats 0.5.0
```

```
## -- Conflicts --------------------------------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
```

```r
library(tidymodels)
```

```
## -- Attaching packages ----------------------------------------------------- tidymodels 0.1.0 --
```

```
## v broom     0.5.5      v rsample   0.0.6 
## v dials     0.0.6      v tune      0.1.0 
## v infer     0.5.1      v workflows 0.1.1 
## v parsnip   0.1.0      v yardstick 0.0.6 
## v recipes   0.1.10
```

```
## -- Conflicts -------------------------------------------------------- tidymodels_conflicts() --
## x scales::discard() masks purrr::discard()
## x dplyr::filter()   masks stats::filter()
## x recipes::fixed()  masks stringr::fixed()
## x dplyr::lag()      masks stats::lag()
## x dials::margin()   masks ggplot2::margin()
## x yardstick::spec() masks readr::spec()
## x recipes::step()   masks stats::step()
```

---

## Ames Iowa Housing Dataset

.left-column[

&amp;nbsp;

&gt; "Data set contains information from the Ames Assessor’s Office used in 
computing assessed values for individual residential properties sold in Ames, 
IA from 2006 to 2010." &amp;mdash; 
[Dataset documentation](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)

&amp;nbsp; 

.font80[

De Cock, Dean. "Ames, Iowa: Alternative to the Boston housing data as an end of 
semester regression project." Journal of Statistics Education 19.3 (2011). 
[URL](http://jse.amstat.org/v19n3/decock.pdf)

]

]

.right-column[


```r
library(AmesHousing)
(ames &lt;- make_ames() %&gt;% select(-matches("Qu")))
```

```
## # A tibble: 2,930 x 74
##    MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape
##    &lt;fct&gt;       &lt;fct&gt;            &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;    
##  1 One_Story_~ Resident~          141    31770 Pave   No_A~ Slightly~
##  2 One_Story_~ Resident~           80    11622 Pave   No_A~ Regular  
##  3 One_Story_~ Resident~           81    14267 Pave   No_A~ Slightly~
##  4 One_Story_~ Resident~           93    11160 Pave   No_A~ Regular  
##  5 Two_Story_~ Resident~           74    13830 Pave   No_A~ Slightly~
##  6 Two_Story_~ Resident~           78     9978 Pave   No_A~ Slightly~
##  7 One_Story_~ Resident~           41     4920 Pave   No_A~ Regular  
##  8 One_Story_~ Resident~           43     5005 Pave   No_A~ Slightly~
##  9 One_Story_~ Resident~           39     5389 Pave   No_A~ Slightly~
## 10 Two_Story_~ Resident~           60     7500 Pave   No_A~ Regular  
## # ... with 2,920 more rows, and 67 more variables:
## #   Land_Contour &lt;fct&gt;, Utilities &lt;fct&gt;, Lot_Config &lt;fct&gt;,
## #   Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;,
## #   Condition_2 &lt;fct&gt;, Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;,
## #   Overall_Cond &lt;fct&gt;, Year_Built &lt;int&gt;, Year_Remod_Add &lt;int&gt;,
## #   Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;, Exterior_1st &lt;fct&gt;,
## #   Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, Mas_Vnr_Area &lt;dbl&gt;,
## #   Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;,
## #   Bsmt_Exposure &lt;fct&gt;, BsmtFin_Type_1 &lt;fct&gt;, BsmtFin_SF_1 &lt;dbl&gt;,
## #   BsmtFin_Type_2 &lt;fct&gt;, BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;,
## #   Total_Bsmt_SF &lt;dbl&gt;, Heating &lt;fct&gt;, Heating_QC &lt;fct&gt;,
## #   Central_Air &lt;fct&gt;, Electrical &lt;fct&gt;, First_Flr_SF &lt;int&gt;,
## #   Second_Flr_SF &lt;int&gt;, Gr_Liv_Area &lt;int&gt;, Bsmt_Full_Bath &lt;dbl&gt;,
## #   Bsmt_Half_Bath &lt;dbl&gt;, Full_Bath &lt;int&gt;, Half_Bath &lt;int&gt;,
## #   Bedroom_AbvGr &lt;int&gt;, Kitchen_AbvGr &lt;int&gt;, TotRms_AbvGrd &lt;int&gt;,
## #   Functional &lt;fct&gt;, Fireplaces &lt;int&gt;, Garage_Type &lt;fct&gt;,
## #   Garage_Finish &lt;fct&gt;, Garage_Cars &lt;dbl&gt;, Garage_Area &lt;dbl&gt;,
## #   Garage_Cond &lt;fct&gt;, Paved_Drive &lt;fct&gt;, Wood_Deck_SF &lt;int&gt;,
## #   Open_Porch_SF &lt;int&gt;, Enclosed_Porch &lt;int&gt;,
## #   Three_season_porch &lt;int&gt;, Screen_Porch &lt;int&gt;, Pool_Area &lt;int&gt;,
## #   Pool_QC &lt;fct&gt;, Fence &lt;fct&gt;, Misc_Feature &lt;fct&gt;, Misc_Val &lt;int&gt;,
## #   Mo_Sold &lt;int&gt;, Year_Sold &lt;int&gt;, Sale_Type &lt;fct&gt;,
## #   Sale_Condition &lt;fct&gt;, Sale_Price &lt;int&gt;, Longitude &lt;dbl&gt;,
## #   Latitude &lt;dbl&gt;
```

]


???

- 2930 observations, 74 variables
- remove quality columns: why?

---

class: center, inverse, middle
name: parsnip

.pull-left70[

&amp;nbsp;

&amp;nbsp;

&amp;nbsp;

# Specify a model with parsnip

]

.pull-right30[

&lt;img src="figures/05-tidymodels//parsnip.png" width="100%" /&gt;

]


---

class: middle

## Specify a model with `parsnip`

.content-box-blue[

.font130[

1. Pick a **model**
2. Set the **engine**
3. Set the **mode** (if needed)

]

]

--

.pull-left[


```r
decision_tree() %&gt;% # model
  set_engine("rpart") %&gt;% # engine
  set_mode("classification") # mode
```

```
## Decision Tree Model Specification (classification)
## 
## Computational engine: rpart
```

]

--

.pull-right[


```r
nearest_neighbor() %&gt;%
  set_engine("kknn") %&gt;%
  set_mode("regression")
```

```
## K-Nearest Neighbor Model Specification (regression)
## 
## Computational engine: kknn
```

]


---

class: middle

All available models are listed at &lt;https://tidymodels.github.io/parsnip/articles/articles/Models.html&gt;.

&lt;iframe src="https://tidymodels.github.io/parsnip/articles/articles/Models.html" width="100%" height="450px"&gt;&lt;/iframe&gt;

---

class: middle

.left-column[

.content-box-blue[

.font130[

1\. Pick a **model**

.fade[

2\. Set the **engine**

3\. Set the **mode**
]


]

]

]

.right-column[

## `linear_reg()`

Specify a model that uses linear regression:


```r
linear_reg(
  mode = "regression", # type of model (only "regression" here)
  penalty = NULL, # amount of regularization
  mixture = NULL # proportion of L1 regularization
)
```


]

???

linear_reg() is a way to generate a specification of a model before fitting and allows the model to be created using different packages in R, Stan, keras, or via Spark. The main arguments for the model are:

penalty: The total amount of regularization in the model. Note that this must be zero for some engines.

mixture: The proportion of L1 regularization in the model. Note that this will be ignored for some engines.

---

class: middle

.left-column[

.content-box-blue[

.font130[

.fade[1\. Pick a **model**]

2\. Set the **engine**

.fade[3\. Set the **mode**]

]

]

]

.right-column[

## `set_engine()`

Add an engine to power or implement the model:


```r
linear_reg() %&gt;% 
* set_engine(engine = "lm", ...)
```

Available engines for `linear_reg()`:

- R: "lm" (the default) or "glmnet"
- Stan: "stan"
- Spark: "spark"
- keras: "keras"

]

---

class: middle

.left-column[

.content-box-blue[

.font130[

.fade[1\. Pick a **model**

2\. Set the **engine**]

3\. Set the **mode**

]

]

]

.right-column[

## `set_mode()`

Set the model type, either `"regression"` or `"classification"`. 
Not necessary if mode is set in Step 1.


```r
linear_reg() %&gt;% 
  set_engine(engine = "lm") %&gt;%
* set_mode(mode = "regression")
```

]


---

## `fit()`

`fit()`: fit a simple linear regression model to predict _sale price_ based on 
_above ground living area_.

.pull-left[


```r
lm_spec &lt;- linear_reg() %&gt;% 
  set_engine(engine = "lm") %&gt;%
  set_mode(mode = "regression")
*m &lt;- fit(
* lm_spec, # parsnip model spec
* Sale_Price ~ Gr_Liv_Area, # formula
* ames # data frame
*) 
m
```

```
## parsnip model object
## 
## Fit time:  0ms 
## 
## Call:
## stats::lm(formula = formula, data = data)
## 
## Coefficients:
## (Intercept)  Gr_Liv_Area  
##     13289.6        111.7
```

]

.pull-right[

&lt;img src="figures/_gen/05-tidymodels/linear-reg-5-1.png" width="425.196850393701" /&gt;


]

???

- fit: fit a model using the parsnip model spec, a formula (lhs: target attribute, rhs: predictors) and the training data

---

## `predict()`

`predict()`: use a fitted model to predict new response values from data. Returns a tibble.

.pull-left[


```r
p &lt;- predict(m, new_data = ames)
p
```

```
## # A tibble: 2,930 x 1
##      .pred
##      &lt;dbl&gt;
##  1 198255.
##  2 113367.
##  3 161731.
##  4 248964.
##  5 195239.
##  6 192447.
##  7 162736.
##  8 156258.
##  9 193787.
## 10 214786.
## # ... with 2,920 more rows
```

]

.pull-right[

&lt;img src="figures/_gen/05-tidymodels/linear-reg-7-1.png" width="425.196850393701" /&gt;


]

???

- residuals: difference between observed and predicted values

---

class: middle, exercise-blue

## Your turn 1

<div class="countdown" id="timer_5e99c3cd" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

1. Create a simple linear regression model on the ames data to predict _sale 
price_ (`Sale_Price`) based on _above ground living area_ (`Gr_Liv_Area`). 
1. Apply the model to the original data.
1. Use `mutate()` to add a column with the observed sale prices; name it `truth`. 

---

class: center, inverse, middle
name: yardstick

.pull-left70[

&amp;nbsp;

&amp;nbsp;

&amp;nbsp;

# Measure model performance with yardstick

]

.pull-right30[

&lt;img src="figures/05-tidymodels//yardstick.png" width="100%" /&gt;

]

---

class: middle

## Measure the model performance with `yardstick::rmse()`

- **Residuals**. The difference between observed and predicted values: `\(\hat{y}_i-y_i\)`.
- **Mean Absolute Error**. `\(\frac{1}{n}\sum_{i=1}^n|\hat{y}_i-y_i|\)`.
- **Root Mean Squared Error**. `\(\sqrt{\frac{1}{n}\sum_{i=1}^n(\hat{y}_i-y_i)^2}\)`.

--

Calculate the RMSE based on two columns in a data frame:

- truth `\(y_i\)`
- predicted estimate `\(\hat{y}\)`



```r
lm_spec &lt;- linear_reg() %&gt;% 
  set_engine(engine = "lm") %&gt;%
  set_mode(mode = "regression")
lm_fit &lt;- fit(object = lm_spec, formula = Sale_Price ~ Gr_Liv_Area, data = ames)
price_pred &lt;- lm_fit %&gt;% 
  predict(new_data = ames) %&gt;% 
  mutate(truth = ames$Sale_Price)

*rmse(price_pred, truth = truth, estimate = .pred)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard      56505.
```

---

## Available metrics in yardstick

&lt;iframe src="https://tidymodels.github.io/yardstick/articles/metric-types.html#metrics" width="100%" height="450px"&gt;&lt;/iframe&gt;

---

class: center, inverse, middle
name: rsample

.pull-left70[

&amp;nbsp;

&amp;nbsp;

&amp;nbsp;

# Perform resampling with rsample

]

.pull-right30[

&lt;img src="figures/05-tidymodels//rsample.png" width="100%" /&gt;

]

???

- so far, we have evaluated model performance on training data which gives us too optimistic estimates of the true model performance
- we need to evaluate the model on a test dataset that is independent from the dataset used for model training 

---

class: middle

## `initial_split()`

`initial_split()`: partition data randomly into a 
single training and a single test set.


```r
set.seed(123)
(ames_split &lt;- initial_split(ames, prop = 3/4)) # prop = proportion of training instances
```

```
## &lt;Training/Validation/Total&gt;
## &lt;2198/732/2930&gt;
```

---

## `training()` and `testing()`

Extract training and testing sets from an `rsplit` object:

.pull-left[


```r
training(ames_split)
```

```
## # A tibble: 2,198 x 74
##    MS_SubClass MS_Zoning Lot_Frontage
##    &lt;fct&gt;       &lt;fct&gt;            &lt;dbl&gt;
##  1 One_Story_~ Resident~          141
##  2 One_Story_~ Resident~           80
##  3 One_Story_~ Resident~           81
##  4 One_Story_~ Resident~           93
##  5 Two_Story_~ Resident~           74
##  6 Two_Story_~ Resident~           78
##  7 One_Story_~ Resident~           41
##  8 Two_Story_~ Resident~           75
##  9 One_Story_~ Resident~            0
## 10 One_Story_~ Resident~           85
## # ... with 2,188 more rows, and 71 more
## #   variables: Lot_Area &lt;int&gt;, Street &lt;fct&gt;,
## #   Alley &lt;fct&gt;, Lot_Shape &lt;fct&gt;,
## #   Land_Contour &lt;fct&gt;, Utilities &lt;fct&gt;,
## #   Lot_Config &lt;fct&gt;, Land_Slope &lt;fct&gt;,
## #   Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;,
## #   Condition_2 &lt;fct&gt;, Bldg_Type &lt;fct&gt;,
## #   House_Style &lt;fct&gt;, Overall_Cond &lt;fct&gt;,
## #   Year_Built &lt;int&gt;, Year_Remod_Add &lt;int&gt;,
## #   Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;,
## #   Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;,
## #   Mas_Vnr_Type &lt;fct&gt;, Mas_Vnr_Area &lt;dbl&gt;,
## #   Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;,
## #   Bsmt_Cond &lt;fct&gt;, Bsmt_Exposure &lt;fct&gt;,
## #   BsmtFin_Type_1 &lt;fct&gt;,
## #   BsmtFin_SF_1 &lt;dbl&gt;,
## #   BsmtFin_Type_2 &lt;fct&gt;,
## #   BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;,
## #   Total_Bsmt_SF &lt;dbl&gt;, Heating &lt;fct&gt;,
## #   Heating_QC &lt;fct&gt;, Central_Air &lt;fct&gt;,
## #   Electrical &lt;fct&gt;, First_Flr_SF &lt;int&gt;,
## #   Second_Flr_SF &lt;int&gt;, Gr_Liv_Area &lt;int&gt;,
## #   Bsmt_Full_Bath &lt;dbl&gt;,
## #   Bsmt_Half_Bath &lt;dbl&gt;, Full_Bath &lt;int&gt;,
## #   Half_Bath &lt;int&gt;, Bedroom_AbvGr &lt;int&gt;,
## #   Kitchen_AbvGr &lt;int&gt;,
## #   TotRms_AbvGrd &lt;int&gt;, Functional &lt;fct&gt;,
## #   Fireplaces &lt;int&gt;, Garage_Type &lt;fct&gt;,
## #   Garage_Finish &lt;fct&gt;, Garage_Cars &lt;dbl&gt;,
## #   Garage_Area &lt;dbl&gt;, Garage_Cond &lt;fct&gt;,
## #   Paved_Drive &lt;fct&gt;, Wood_Deck_SF &lt;int&gt;,
## #   Open_Porch_SF &lt;int&gt;,
## #   Enclosed_Porch &lt;int&gt;,
## #   Three_season_porch &lt;int&gt;,
## #   Screen_Porch &lt;int&gt;, Pool_Area &lt;int&gt;,
## #   Pool_QC &lt;fct&gt;, Fence &lt;fct&gt;,
## #   Misc_Feature &lt;fct&gt;, Misc_Val &lt;int&gt;,
## #   Mo_Sold &lt;int&gt;, Year_Sold &lt;int&gt;,
## #   Sale_Type &lt;fct&gt;, Sale_Condition &lt;fct&gt;,
## #   Sale_Price &lt;int&gt;, Longitude &lt;dbl&gt;,
## #   Latitude &lt;dbl&gt;
```

]

.pull-right[

```r
testing(ames_split)
```

```
## # A tibble: 732 x 74
##    MS_SubClass MS_Zoning Lot_Frontage
##    &lt;fct&gt;       &lt;fct&gt;            &lt;dbl&gt;
##  1 One_Story_~ Resident~           43
##  2 One_Story_~ Resident~           39
##  3 Two_Story_~ Resident~           60
##  4 Two_Story_~ Resident~           63
##  5 Two_Story_~ Resident~           47
##  6 One_Story_~ Resident~           88
##  7 One_Story_~ Resident~            0
##  8 Two_Story_~ Resident~           21
##  9 One_Story_~ Resident~           95
## 10 One_Story_~ Resident~           70
## # ... with 722 more rows, and 71 more
## #   variables: Lot_Area &lt;int&gt;, Street &lt;fct&gt;,
## #   Alley &lt;fct&gt;, Lot_Shape &lt;fct&gt;,
## #   Land_Contour &lt;fct&gt;, Utilities &lt;fct&gt;,
## #   Lot_Config &lt;fct&gt;, Land_Slope &lt;fct&gt;,
## #   Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;,
## #   Condition_2 &lt;fct&gt;, Bldg_Type &lt;fct&gt;,
## #   House_Style &lt;fct&gt;, Overall_Cond &lt;fct&gt;,
## #   Year_Built &lt;int&gt;, Year_Remod_Add &lt;int&gt;,
## #   Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;,
## #   Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;,
## #   Mas_Vnr_Type &lt;fct&gt;, Mas_Vnr_Area &lt;dbl&gt;,
## #   Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;,
## #   Bsmt_Cond &lt;fct&gt;, Bsmt_Exposure &lt;fct&gt;,
## #   BsmtFin_Type_1 &lt;fct&gt;,
## #   BsmtFin_SF_1 &lt;dbl&gt;,
## #   BsmtFin_Type_2 &lt;fct&gt;,
## #   BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;,
## #   Total_Bsmt_SF &lt;dbl&gt;, Heating &lt;fct&gt;,
## #   Heating_QC &lt;fct&gt;, Central_Air &lt;fct&gt;,
## #   Electrical &lt;fct&gt;, First_Flr_SF &lt;int&gt;,
## #   Second_Flr_SF &lt;int&gt;, Gr_Liv_Area &lt;int&gt;,
## #   Bsmt_Full_Bath &lt;dbl&gt;,
## #   Bsmt_Half_Bath &lt;dbl&gt;, Full_Bath &lt;int&gt;,
## #   Half_Bath &lt;int&gt;, Bedroom_AbvGr &lt;int&gt;,
## #   Kitchen_AbvGr &lt;int&gt;,
## #   TotRms_AbvGrd &lt;int&gt;, Functional &lt;fct&gt;,
## #   Fireplaces &lt;int&gt;, Garage_Type &lt;fct&gt;,
## #   Garage_Finish &lt;fct&gt;, Garage_Cars &lt;dbl&gt;,
## #   Garage_Area &lt;dbl&gt;, Garage_Cond &lt;fct&gt;,
## #   Paved_Drive &lt;fct&gt;, Wood_Deck_SF &lt;int&gt;,
## #   Open_Porch_SF &lt;int&gt;,
## #   Enclosed_Porch &lt;int&gt;,
## #   Three_season_porch &lt;int&gt;,
## #   Screen_Porch &lt;int&gt;, Pool_Area &lt;int&gt;,
## #   Pool_QC &lt;fct&gt;, Fence &lt;fct&gt;,
## #   Misc_Feature &lt;fct&gt;, Misc_Val &lt;int&gt;,
## #   Mo_Sold &lt;int&gt;, Year_Sold &lt;int&gt;,
## #   Sale_Type &lt;fct&gt;, Sale_Condition &lt;fct&gt;,
## #   Sale_Price &lt;int&gt;, Longitude &lt;dbl&gt;,
## #   Latitude &lt;dbl&gt;
```


]

---

class: exercise-blue, middle

## Your turn 2

<div class="countdown" id="timer_5e99c5de" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">04</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

Use `initial_split()`, `training()`, `testing()`, `lm()` and `rmse()` to:

1. Split ames into training (70%) and test (30%) sets. Save the `rsplit`.
1. Extract the training data. Fit a linear model to it. Save the model as `lm_fit`.
1. Measure the RMSE of your linear model with your test set.  

Keep `set.seed(100)` at the start of your code.

&lt;!-- .font70[RMSE=54721]  --&gt;

---

## Stratified sampling


```r
*initial_split(ames, strata = Sale_Price, breaks = 6)
```

&lt;img src="figures/_gen/05-tidymodels/strat-sampling-1-1.png" width="708.661417322835" /&gt;

???

- apply equal-frequency binning on the target variable and draw train/test 
instances with the specified split percentages from each bin 
- to ensure that we have (approx.) the same ratio of train/test instances in each bin

General drawback of holdout method:

- If testing set is small, performance metrics may be unreliable
- If training set is small, model fit may be poor

-&gt; resampling

---

## Cross-validation with `vfold_cv()`

General syntax: 


```r
vfold_cv(data, v = 10, repeats = 1, strata = NULL, breaks = 4, ...)
```

--

.pull-left60[

Example: 10-fold CV on ames data:


```r
set.seed(123)
(folds &lt;- vfold_cv(ames, v = 5))
```

```
## #  5-fold cross-validation 
## # A tibble: 5 x 2
##   splits             id   
##   &lt;named list&gt;       &lt;chr&gt;
## 1 &lt;split [2.3K/586]&gt; Fold1
## 2 &lt;split [2.3K/586]&gt; Fold2
## 3 &lt;split [2.3K/586]&gt; Fold3
## 4 &lt;split [2.3K/586]&gt; Fold4
## 5 &lt;split [2.3K/586]&gt; Fold5
```

Check whether mean `\(y\)` is approx. equal in each training fold: 


```r
map_dbl(folds$splits, ~mean(.x$data$Sale_Price[.x$in_id]))
```

```
##        1        2        3        4        5 
## 181310.8 180991.0 180840.0 181268.6 179569.9
```

]

.pull-right40[

&lt;img src="figures/_gen/05-tidymodels/cv-4-1.png" width="425.196850393701" /&gt;

]

???

- `vfold_cv()` also has a strata argument

---

## Calculate the model performance on multiple resamples with `fit_resamples()`


```r
res &lt;- fit_resamples(lm_spec, Sale_Price ~ Gr_Liv_Area, resamples = folds)
res
```

```
## #  5-fold cross-validation 
## # A tibble: 5 x 4
##   splits             id    .metrics         .notes          
##   &lt;list&gt;             &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          
## 1 &lt;split [2.3K/586]&gt; Fold1 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt;
## 2 &lt;split [2.3K/586]&gt; Fold2 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt;
## 3 &lt;split [2.3K/586]&gt; Fold3 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt;
## 4 &lt;split [2.3K/586]&gt; Fold4 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt;
## 5 &lt;split [2.3K/586]&gt; Fold5 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt;
```

???

- we need fit_resamples because we have more than 1 split
- returns a tibble with 5 rows (number of resamples)
- several list columns (add pull, pluck)
- `splits`: info on training and test set assignment in resample
- `.metrics`: model performance
- `.notes`: contains information in case an error has occurred

---

## Collapse performance results across resamples with `collect_metrics()`


```r
res %&gt;% collect_metrics()
```

```
## # A tibble: 2 x 5
##   .metric .estimator      mean     n   std_err
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;
## 1 rmse    standard   56486.        5 1866.    
## 2 rsq     standard       0.504     5    0.0193
```

```r
res %&gt;% collect_metrics(summarize = FALSE)
```

```
## # A tibble: 10 x 4
##    id    .metric .estimator .estimate
##    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
##  1 Fold1 rmse    standard   51064.   
##  2 Fold1 rsq     standard       0.542
##  3 Fold2 rmse    standard   57206.   
##  4 Fold2 rsq     standard       0.464
##  5 Fold3 rmse    standard   53526.   
##  6 Fold3 rsq     standard       0.557
##  7 Fold4 rmse    standard   61210.   
##  8 Fold4 rsq     standard       0.468
##  9 Fold5 rmse    standard   59422.   
## 10 Fold5 rsq     standard       0.488
```

???

- `collect_metrics`: helper function to expand the `.metrics` column
- if summarize = TRUE (default), it averages across all folds
- this code is the same as res %&gt;% collect_metrics(summarize = FALSE): 
unnest(res %&gt;% select(id, .metrics), cols = .metrics)


---

## `metric_set()`

`metric_set()`: a helper function for selecting yardstick metric functions.

.pull-left[


```r
fit_resamples(
  object, 
  resamples, 
  ..., 
* metrics = metric_set(rmse, rsq),
  control = control_resamples()
)
```

]

.pull-right[

.content-box-blue[

If `metrics = NULL`:

- regression: `metric_set(rmse, rsq)`
- classification: `metric_set(accuracy, roc_auc)`

]

]

???

- rmse and rsq are functions

---

class: exercise-blue, middle

## Your turn 3

<div class="countdown" id="timer_5e99c40d" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

Modify the code below to return the **Mean Absolute Error** and **R²**. 
Visit 
&lt;https://tidymodels.github.io/yardstick/reference/index.html#section-regression-metrics&gt; 
to find the right function to use.


```r
set.seed(100)

folds &lt;- vfold_cv(ames, v = 5)

fit_res &lt;- fit_resamples(Sale_Price ~ Gr_Liv_Area,
                         model = lm_spec,
                         resamples = folds)

collect_metrics(fit_res, summarize = TRUE)
```

---

## Other resampling methods

- `loo_cv()`: leave-one-out CV
- `mc_cv()`: repeated holdout / Monte Carlo (random) CV: test sets sampled 
without replacement
- `bootstraps()`: test sets sampled with replacement

&lt;img src="figures/_gen/05-tidymodels/rsample-other-resampling-1.png" width="963.779527559055" /&gt;

---

class: exercise-blue, middle

## Your turn 4

<div class="countdown" id="timer_5e99c4a3" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

Compare the performance of a regression tree model with a k-nearest neighbor 
model on 10 bootstrap samples using RMSE as evaluation measure. 

Set `rpart` as engine for the decision tree model and `kknn` as engine for the 
k-nearest neighbor model. Leave all model-specific parameters (e.g. `tree_depth` 
and `neighbors`) at default.

Which of the two methods perform better and why? 

---

## A classification example


```r
stackoverflow &lt;- read_rds(here::here("datasets/stackoverflow.rds"))
glimpse(stackoverflow)
```

```
## Rows: 1,150
## Columns: 21
## $ country                              &lt;fct&gt; United States, United States, United Kin...
## $ salary                               &lt;dbl&gt; 63750.00, 93000.00, 40625.00, 45000.00, ...
## $ years_coded_job                      &lt;int&gt; 4, 9, 8, 3, 8, 12, 20, 17, 20, 4, 3, 13,...
## $ open_source                          &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0...
## $ hobby                                &lt;dbl&gt; 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1...
## $ company_size_number                  &lt;dbl&gt; 20, 1000, 10000, 1, 10, 100, 20, 500, 1,...
## $ remote                               &lt;fct&gt; Remote, Remote, Remote, Remote, Remote, ...
## $ career_satisfaction                  &lt;int&gt; 8, 8, 5, 10, 8, 10, 9, 7, 8, 7, 9, 8, 8,...
## $ data_scientist                       &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ database_administrator               &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0...
## $ desktop_applications_developer       &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1...
## $ developer_with_stats_math_background &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0...
## $ dev_ops                              &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0...
## $ embedded_developer                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1...
## $ graphic_designer                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ graphics_programming                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ machine_learning_specialist          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ mobile_developer                     &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0...
## $ quality_assurance_engineer           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ systems_administrator                &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0...
## $ web_developer                        &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0...
```

.font80[Data source: [Stack Overflow Annual Developer Survey](https://insights.stackoverflow.com/survey)]


???

- what makes a developer more likely to work remotely? 
- Developers can work in their company offices or they can work remotely, and it turns out that there are specific characteristics of developers, such as the size of the company that they work for, how much experience they have, or where in the world they live, that affect how likely they are to be a remote developer.

---

class: middle

## Specify a classification model


.left-column[

.content-box-blue[

.font130[

1\. Pick a **model**

2\. Set the **engine**

3\. Set the **mode**

]

]

]

--

.right-column[

Specify a decision tree model with default parameter settings:


```r
vanilla_tree_spec &lt;- decision_tree() %&gt;% 
  set_engine("rpart") %&gt;% 
* set_mode("classification")
```


]

---

class: middle

Measure the performance of a vanilla decision tree model using 5-fold CV: 


```r
set.seed(100)
so_cv &lt;- vfold_cv(stackoverflow, v = 5)
(fit_van_res &lt;- fit_resamples(vanilla_tree_spec, remote ~ ., resamples = so_cv) %&gt;% 
  collect_metrics())
```

```
## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.642     5  0.0107
## 2 roc_auc  binary     0.660     5  0.0180
```

--

&amp;#x1F914; _"Can we improve the performance by tuning the algorithm parameters?"_

--

&amp;#x1F914; _"Which parameters can we tune?"_

---

class: middle

## args()

`args()` prints the arguments for a parsnip model specification:


```r
args(decision_tree)
```

```
## function (mode = "unknown", cost_complexity = NULL, tree_depth = NULL, 
##     min_n = NULL) 
## NULL
```

--

Arguments of `decision_tree()`:

- `cost_complexity`: minimum fit improvement of a split (0 &lt; `cost_complexity` `\(\leq\)` 1)
- `tree_depth`: maximum number of levels in the tree
- `min_n`: minimum number of observations in a node in order for a split to be attempted

---

class: middle


```r
decision_tree(
  cost_complexity = 0.01,  # min. fit improvement of a split (0 &lt; cp &lt;=1)
  tree_depth = 30, # max. number of levels in the tree
  min_n = 20 # min. number of observations in a node in order for a split to be attempted
)
```

```
## Decision Tree Model Specification (unknown)
## 
## Main Arguments:
##   cost_complexity = 0.01
##   tree_depth = 30
##   min_n = 20
```

--

If the arguments are left to their defaults (`NULL`), the arguments will use the 
engine's underlying model functions default value.

For example, `rpart` is used as default engine. The default parameters are:


```r
args(rpart::rpart.control) # cost_complexity -&gt; cp; tree_depth -&gt; maxdepth; min_n -&gt; minsplit
```

```
## function (minsplit = 20L, minbucket = round(minsplit/3), cp = 0.01, 
##     maxcompete = 4L, maxsurrogate = 5L, usesurrogate = 2L, xval = 10L, 
##     surrogatestyle = 0L, maxdepth = 30L, ...) 
## NULL
```

---

class: middle

## `set_args()`

`set_args()`: **change** the arguments for a parsnip model specification:


```r
dt_spec &lt;- decision_tree()

dt_spec %&gt;% set_args(tree_depth = 3)
```

```
## Decision Tree Model Specification (unknown)
## 
## Main Arguments:
##   tree_depth = 3
```

--

.pull-left[

... which is equivalent to:


```r
dt_spec &lt;- decision_tree(tree_depth = 3)
dt_spec
```

```
## Decision Tree Model Specification (unknown)
## 
## Main Arguments:
##   tree_depth = 3
```

]

--

.pull-right[

An example spec of model, engine, mode and tree depth:


```r
decision_tree() %&gt;%
  set_engine("rpart") %&gt;% 
  set_mode("classification") %&gt;%
  set_args(tree_depth = 3)
```

```
## Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   tree_depth = 3
## 
## Computational engine: rpart
```

]

---

class: middle, center

&lt;img src="figures/_gen/05-tidymodels/cp-1-1.png" width="1020.47244094488" /&gt;

???

&lt;!-- parsimonious model --&gt;

---

class: middle, center 

&lt;img src="figures/_gen/05-tidymodels/cp-2-1.png" width="1020.47244094488" /&gt;

---

class: middle, center

.pull-left[

Overfitted tree (`cost_complexity`=0.0008):

&lt;img src="figures/_gen/05-tidymodels/rpart-overfitted-1.png" width="425.196850393701" /&gt;

]

.pull-right[

Optimal tree (`cost_complexity`=0.0093):

&lt;img src="figures/_gen/05-tidymodels/rpart-ideal-size-1.png" width="425.196850393701" /&gt;

]

---

class: exercise-blue, middle

## Your turn 5

Create a new classification tree model specification; name it `big_tree_spec`. 
Set the cost complexity to 0, and the minimum number of data points in a node to split to 1.

Compare the metrics of the big tree to the vanilla tree. 
Which one predicts the test set better?

Vanilla tree performance: 

- accuracy = 0.64
- ROC AUC = 0.66

---

class: middle

## `workflow()`

Create a workflow with `workflow()`.

???

- to perform hyperparameter tuning, we need to create a workflow object
- workflow: bundle together preprocessing, modeling and postprocessing
- easier to see the benefits of workflows with examples...

--

## `add_formula()`

Add a formula to a workflow

`workflow() %&gt;% add_formula(Sale_Price ~ Year)`

--

## `add_model()`

Add a parsnip model spec to a workflow:

`workflow() %&gt;% add_model(lm_spec)`

---

## Example workflow

&lt;!-- # tree &lt;- fit(wf, stackoverflow) %&gt;% pull_workflow_fit() --&gt;


```r
wf &lt;- workflow() %&gt;%
  add_formula(remote ~ .) %&gt;%
  add_model(decision_tree() %&gt;% set_engine("rpart") %&gt;% set_mode("classification")) 

wf %&gt;% fit_resamples(so_cv)
```

```
## #  5-fold cross-validation 
## # A tibble: 5 x 4
##   splits            id    .metrics         .notes          
##   &lt;list&gt;            &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          
## 1 &lt;split [920/230]&gt; Fold1 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt;
## 2 &lt;split [920/230]&gt; Fold2 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt;
## 3 &lt;split [920/230]&gt; Fold3 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt;
## 4 &lt;split [920/230]&gt; Fold4 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt;
## 5 &lt;split [920/230]&gt; Fold5 &lt;tibble [2 x 3]&gt; &lt;tibble [0 x 1]&gt;
```

???

- we do not need to specify a formula within the fitting function

---

class: middle 

## `update_formula()`

Replace a workflow formula with a new one:


```r
workflow() %&gt;% 
  add_formula(remote ~ .) %&gt;%
* update_formula(remote ~ salary + open_source)
```

```
## == Workflow ===================================================================================
## Preprocessor: Formula
## Model: None
## 
## -- Preprocessor -------------------------------------------------------------------------------
## remote ~ salary + open_source
```

---

class: middle

## `update_model()`

Replaces a workflow model spec with a new one:


```r
workflow() %&gt;% 
  add_model(nearest_neighbor()) %&gt;%
  update_model(decision_tree())
```

```
## == Workflow ===================================================================================
## Preprocessor: None
## Model: decision_tree()
## 
## -- Model --------------------------------------------------------------------------------------
## Decision Tree Model Specification (unknown)
```

---

class: center, inverse, middle

.pull-left70[

&amp;nbsp;

&amp;nbsp;

&amp;nbsp;

# Tune model hyperparameters with tune

]

.pull-right30[

&lt;img src="figures/05-tidymodels//tune.png" width="100%" /&gt;

]


---

class: middle

## `tune()`

`tune()` is a placeholder for hyperparameters tat are to be tuned:


```r
decision_tree(cost_complexity = tune())
```

```
## Decision Tree Model Specification (unknown)
## 
## Main Arguments:
##   cost_complexity = tune()
```

---

## `tune_grid()`

A version of `fit_resamples()` that performs a grid search for the best combination of tuned hyper-parameters.


```r
tune_grid(
  object, # a model workflow, R formula or recipe object. 
  resamples, # a resampling object, e.g. the output of vfold_cv() 
  ..., 
  grid = 10, # the number of tuning iterations or a data frame of tuning operations (tuning grid)
  metrics = NULL, # yardstick::metric_set() or NULL
  control = control_grid() # An object used to modify the tuning process
)
```

???

recipes will be discussed later

---

class: middle

## `expand_grid()`

`tidyr::expand_grid()`: takes one or more vectors, and returns a data frame 
holding all combinations of their values.


```r
expand_grid(cost_complexity = 10^(0:-5), min_n = seq(4,20,4))
```

```
## # A tibble: 30 x 2
##    cost_complexity min_n
##              &lt;dbl&gt; &lt;dbl&gt;
##  1             1       4
##  2             1       8
##  3             1      12
##  4             1      16
##  5             1      20
##  6             0.1     4
##  7             0.1     8
##  8             0.1    12
##  9             0.1    16
## 10             0.1    20
## # ... with 20 more rows
```

.footnote[`expand_grid()` is a re-implementation of the base `expand.grid()`.]

---

class: middle


```r
dt_spec &lt;- decision_tree(
* cost_complexity = tune(),
* tree_depth = tune()
) %&gt;% 
  set_engine("rpart") %&gt;% 
  set_mode("classification")

dt_wf &lt;- workflow() %&gt;% 
  add_model(dt_spec) %&gt;%
  add_formula(remote ~ .)

dt_res &lt;- dt_wf %&gt;%
    tune_grid(resamples = so_cv, 
*             grid = expand_grid(cost_complexity = 10^-(1:5), tree_depth = 1:6)
    )
dt_res
```

```
## #  5-fold cross-validation 
## # A tibble: 5 x 4
##   splits            id    .metrics          .notes          
##   &lt;list&gt;            &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          
## 1 &lt;split [920/230]&gt; Fold1 &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 2 &lt;split [920/230]&gt; Fold2 &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 3 &lt;split [920/230]&gt; Fold3 &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 4 &lt;split [920/230]&gt; Fold4 &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 5 &lt;split [920/230]&gt; Fold5 &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt;
```

???

1. specify parsnip model
1. create workflow, add parsnip model and the formula
1. invoke `tune_grid()` on the workflow and the tuning grid we create with 
`expand_grid()`

- `dt_res`: performance for each fold stored in list column `.metrics`

---

class: middle


```r
dt_res %&gt;% 
  collect_metrics() %&gt;%
  filter(.metric == "accuracy") %&gt;%
  arrange(desc(mean))
```

```
## # A tibble: 30 x 7
##    cost_complexity tree_depth .metric  .estimator  mean     n std_err
##              &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
##  1         0.00001          2 accuracy binary     0.66      5  0.0158
##  2         0.0001           2 accuracy binary     0.66      5  0.0158
##  3         0.001            2 accuracy binary     0.66      5  0.0158
##  4         0.01             2 accuracy binary     0.656     5  0.0142
##  5         0.00001          1 accuracy binary     0.643     5  0.0114
##  6         0.0001           1 accuracy binary     0.643     5  0.0114
##  7         0.001            1 accuracy binary     0.643     5  0.0114
##  8         0.01             1 accuracy binary     0.643     5  0.0114
##  9         0.1              1 accuracy binary     0.643     5  0.0114
## 10         0.1              2 accuracy binary     0.643     5  0.0114
## # ... with 20 more rows
```



---

class: middle

## `show_best()`

`show_best()`: display the `n` best hyperparameters combinations according to 
a `metric`:


```r
dt_res %&gt;% 
  show_best(metric = "accuracy", n = 5)
```

```
## # A tibble: 5 x 7
##   cost_complexity tree_depth .metric  .estimator  mean     n std_err
##             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1         0.00001          2 accuracy binary     0.66      5  0.0158
## 2         0.0001           2 accuracy binary     0.66      5  0.0158
## 3         0.001            2 accuracy binary     0.66      5  0.0158
## 4         0.01             2 accuracy binary     0.656     5  0.0142
## 5         0.00001          1 accuracy binary     0.643     5  0.0114
```

---

class: middle

## `autoplot()`

`autoplot()`: quickly visualize tuning results



```r
dt_res %&gt;% autoplot()
```

&lt;img src="figures/_gen/05-tidymodels/dt-tune-4-autoplot-1.png" width="623.622047244095" style="display: block; margin: auto;" /&gt;

---

class: middle

## `select_best()`

`select_best()` returns the best combination of hyperparameters according to 
a metric:


```r
so_best &lt;- dt_res %&gt;% select_best(metric = "roc_auc")
so_best
```

```
## # A tibble: 1 x 2
##   cost_complexity tree_depth
##             &lt;dbl&gt;      &lt;int&gt;
## 1         0.00001          2
```

???

- returns the first combination in case of ties


---

class: middle

## `finalize_workflow()`

`finalize_workflow()`: replaces `tune()` placeholders in a model/recipe/workflow 
with a set of hyper-parameter values.


```r
dt_wf_final &lt;- dt_wf %&gt;% finalize_workflow(so_best) 
dt_wf_final
```

```
## == Workflow ===================================================================================
## Preprocessor: Formula
## Model: decision_tree()
## 
## -- Preprocessor -------------------------------------------------------------------------------
## remote ~ .
## 
## -- Model --------------------------------------------------------------------------------------
## Decision Tree Model Specification (classification)
## 
## Main Arguments:
*##   cost_complexity = 0.00001
*##   tree_depth = 2
## 
## Computational engine: rpart
```

???

last_fit()




vignettes of a package?
vignette(package="grid")

grid_random(cost_complexity(), tree_depth())

---

class: exercise-blue

## Your Turn 6

<div class="countdown" id="timer_5e99c602" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

Create a KNN workflow on the ames houce prices problem that tunes over the 
number of neighbors in a 5-fold stratified CV scheme. 
The candidate values of `neighbors` are `1, 11, 21, 31, ..., 101`.

???



---

class: center, middle

&lt;img src="figures/_gen/05-tidymodels/yt6-sol-1.png" width="708.661417322835" /&gt;

---

class: inverse, middle

.pull-left70[

&amp;nbsp;

&amp;nbsp;

# Preprocessing with recipes

]

.pull-right30[

&lt;img src="figures/05-tidymodels//recipes.png" width="100%" /&gt;

]


---

class: middle

.content-box-blue[
.font130[
1\. Create a `recipe()`

2\. Define the predictor and outcome variables

3\. Add one or more preprocessing step _specifications_ 

4\. Calculate statistics from the training set

5\. Apply preprocessing to datasets
]
]

---

class: middle

.left-column[

&amp;nbsp;

.content-box-blue[
.font130[
1\. Create a `recipe()`

2\. Define the predictor and outcome variables

.fade[

3\. Add one or more preprocessing step _specifications_

4\. Calculate statistics from the training set

5\. Apply preprocessing to datasets

]
]
]
]

.right-column[

## recipe()

`recipe()`: create a recipe by specifying predictors, responses and reference (_template_) data frame. 


```r
*recipe(Sale_Price ~ ., data = ames)
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         73
```

]


---

class: middle

.left-column[

&amp;nbsp;

&amp;nbsp;

.content-box-blue[
.font130[
.fade[1\. Create a `recipe()`]

.fade[2\. Define the predictor and outcome variables]

3\. Add one or more preprocessing step _specifications_

.fade[

4\. Calculate statistics from the training set

5\. Apply preprocessing to datasets

]

]
]
]

.right-column[

## step_*()

`step_*()`: add preprocessing step specifications in the order they will be performed.


```r
recipe(Sale_Price ~ ., data = ames) %&gt;%
  # step_novel(): assign a previously unseen factor level to 
  # a new value
* step_novel(all_nominal()) %&gt;%
  # step_zv(): zero variance filter: remove vars that contain 
  # only a single value
* step_zv(all_predictors())
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         73
## 
## Operations:
## 
## Novel factor level assignment for all_nominal
## Zero variance filter on all_predictors
```

]



???

- How does recipes know what is a predictor and what is an outcome? &gt; formula
- How does recipes know what is numeric and what is nominal? &gt; data argument


Preprocessing and Feature Engineering

This part mostly concerns what we can do to our variables to make the models more effective.

This is mostly related to the predictors. Operations that we might use are:

    transformations of individual predictors or groups of variables

    alternate encodings of a variable

    elimination of predictors (unsupervised)

In statistics, this is generally called preprocessing the data. As usual, the computer science side of modeling has a much flashier name: feature engineering. 

Reasons for Modifying the Data

    Some models (K-NN, SVMs, PLS, neural networks) require that the predictor variables have the same units. Centering and scaling the predictors can be used for this purpose.

    Other models are very sensitive to correlations between the predictors and filters or PCA signal extraction can improve the model.

    As we'll see in an example, changing the scale of the predictors using a transformation can lead to a big improvement.

    In other cases, the data can be encoded in a way that maximizes its effect on the model. Representing the date as the day of the week can be very effective for modeling public transportation data.


    Many models cannot cope with missing data so imputation strategies might be necessary.

    Development of new features that represent something important to the outcome (e.g. compute distances to public transportation, university buildings, public schools, etc.)


---

## step_*()

Complete list at: &lt;https://tidymodels.github.io/recipes/reference/index.html&gt;

&lt;iframe src="https://tidymodels.github.io/recipes/reference/index.html#section-step-functions-imputation" width="100%" height="450px"&gt;&lt;/iframe&gt;

---

## Selectors

**Selectors**, e.g., `all_nominal()` and `all_predictors()` are helper functions for 
selecting sets of variables, which behave similar to the select helpers from `dplyr`.


```r
rec %&gt;% 
  step_novel(all_nominal()) %&gt;%
  step_zv(all_predictors())
```

--

&amp;nbsp;

.font130[


|selector                  |description                                     |
|:-------------------------|:-----------------------------------------------|
|`all_predictors()`        |Each x variable  (right side of ~)              |
|`all_outcomes()`          |Each y variable  (left side of ~)               |
|`all_numeric()`           |Each numeric variable                           |
|`all_nominal()`           |Each categorical variable (e.g. factor, string) |
|`dplyr::select()` helpers |`starts_with('Lot_')`, etc.                     |


]

---

class: exercise-blue, middle

## Your Turn 7

<div class="countdown" id="timer_5e99c418" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

Create a recipe that performs z-score normalization on each numeric 
variable of the ames data. 

A z-score is calculated by subtracting the variable mean from an individual 
raw value and then dividing the difference by the variable standard deviation.

Tip: Look up the appropriate `step_*()` functions at 
&lt;https://tidymodels.github.io/recipes/reference/index.html&gt;.

---

class: middle

.left-column[

&amp;nbsp;

&amp;nbsp;

.content-box-blue[
.font130[
.fade[1\. Create a `recipe()`]

.fade[2\. Define the predictor and outcome variables]

.fade[3\. Add one or more preprocessing step _specifications_]

4\. Calculate statistics from the training set

.fade[5\. Apply preprocessing to datasets]

]
]
]

.right-column[

## `prep()`

`prep()` "trains" a recipe, i.e., calculates statistics from the training data


```r
recipe(Sale_Price ~ ., data = ames) %&gt;%
  step_novel(all_nominal()) %&gt;%
  step_zv(all_predictors()) %&gt;%
* prep(training = training(ames_split))
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         73
## 
## Training data contained 2198 data points and no missing data.
## 
## Operations:
## 
## Novel factor level assignment for MS_SubClass, MS_Zoning, Street, Alley, ... [trained]
## Zero variance filter removed no terms [trained]
```

]

---

class: middle

.left-column[

&amp;nbsp;

&amp;nbsp;

.content-box-blue[
.font130[
.fade[1\. Create a `recipe()`]

.fade[2\. Define the predictor and outcome variables]

.fade[3\. Add one or more preprocessing step _specifications_]

.fade[4\. Calculate statistics from the training set]

5\. Apply preprocessing to datasets

]
]
]

.right-column[

## `bake()`

`bake()` transforms data with the prepped recipe


```r
recipe(Sale_Price ~ ., data = ames) %&gt;%
  step_novel(all_nominal()) %&gt;%
  step_zv(all_predictors()) %&gt;%
  prep(training = training(ames_split)) %&gt;%
* bake(new_data = testing(ames_split)) # or training(ames_split)
```

```
## # A tibble: 732 x 74
##    MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape Land_Contour
##    &lt;fct&gt;       &lt;fct&gt;            &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt;       
##  1 One_Story_~ Resident~           43     5005 Pave   No_A~ Slightly~ HLS         
##  2 One_Story_~ Resident~           39     5389 Pave   No_A~ Slightly~ Lvl         
##  3 Two_Story_~ Resident~           60     7500 Pave   No_A~ Regular   Lvl         
##  4 Two_Story_~ Resident~           63     8402 Pave   No_A~ Slightly~ Lvl         
##  5 Two_Story_~ Resident~           47    53504 Pave   No_A~ Moderate~ HLS         
##  6 One_Story_~ Resident~           88    11394 Pave   No_A~ Regular   Lvl         
##  7 One_Story_~ Resident~            0    11241 Pave   No_A~ Slightly~ Lvl         
##  8 Two_Story_~ Resident~           21     1680 Pave   No_A~ Regular   Lvl         
##  9 One_Story_~ Resident~           95    12182 Pave   No_A~ Regular   Lvl         
## 10 One_Story_~ Resident~           70    10171 Pave   No_A~ Slightly~ Lvl         
## # ... with 722 more rows, and 66 more variables: Utilities &lt;fct&gt;, Lot_Config &lt;fct&gt;,
## #   Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;, Condition_2 &lt;fct&gt;,
## #   Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, Overall_Cond &lt;fct&gt;, Year_Built &lt;int&gt;,
## #   Year_Remod_Add &lt;int&gt;, Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;, Exterior_1st &lt;fct&gt;,
## #   Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;fct&gt;,
## #   Foundation &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, Bsmt_Exposure &lt;fct&gt;, BsmtFin_Type_1 &lt;fct&gt;,
## #   BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_Type_2 &lt;fct&gt;, BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;,
## #   Total_Bsmt_SF &lt;dbl&gt;, Heating &lt;fct&gt;, Heating_QC &lt;fct&gt;, Central_Air &lt;fct&gt;,
## #   Electrical &lt;fct&gt;, First_Flr_SF &lt;int&gt;, Second_Flr_SF &lt;int&gt;, Gr_Liv_Area &lt;int&gt;,
## #   Bsmt_Full_Bath &lt;dbl&gt;, Bsmt_Half_Bath &lt;dbl&gt;, Full_Bath &lt;int&gt;, Half_Bath &lt;int&gt;,
## #   Bedroom_AbvGr &lt;int&gt;, Kitchen_AbvGr &lt;int&gt;, TotRms_AbvGrd &lt;int&gt;, Functional &lt;fct&gt;,
## #   Fireplaces &lt;int&gt;, Garage_Type &lt;fct&gt;, Garage_Finish &lt;fct&gt;, Garage_Cars &lt;dbl&gt;,
## #   Garage_Area &lt;dbl&gt;, Garage_Cond &lt;fct&gt;, Paved_Drive &lt;fct&gt;, Wood_Deck_SF &lt;int&gt;,
## #   Open_Porch_SF &lt;int&gt;, Enclosed_Porch &lt;int&gt;, Three_season_porch &lt;int&gt;,
## #   Screen_Porch &lt;int&gt;, Pool_Area &lt;int&gt;, Pool_QC &lt;fct&gt;, Fence &lt;fct&gt;, Misc_Feature &lt;fct&gt;,
## #   Misc_Val &lt;int&gt;, Mo_Sold &lt;int&gt;, Year_Sold &lt;int&gt;, Sale_Type &lt;fct&gt;,
## #   Sale_Condition &lt;fct&gt;, Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, Sale_Price &lt;int&gt;
```

]

???

actually, you don't need to do this! The fit functions do it for you

---

class: bottom, right

background-image: url("figures/05-tidymodels/recipes-workflow.png")
background-size: contain

.font70[[Source](https://twitter.com/allison_horst/status/1159809527023198209?s=20)]

---

## `juice()`

`juice()` returns the preprocessed training data back from a prepped recipe, 
without having to rerun the preprocessing steps on the training data. 


```r
rec &lt;- recipe(Sale_Price ~ ., data = ames) %&gt;% 
    step_center(all_numeric()) %&gt;% 
    step_scale(all_numeric())
rec %&gt;% 
  prep(training = training(ames_split), 
*      retain = TRUE
  ) %&gt;% 
* juice()
```

```
## # A tibble: 2,198 x 74
##    MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape Land_Contour
##    &lt;fct&gt;       &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt;       
##  1 One_Story_~ Resident~        2.46   2.64    Pave   No_A~ Slightly~ Lvl         
##  2 One_Story_~ Resident~        0.658  0.185   Pave   No_A~ Regular   Lvl         
##  3 One_Story_~ Resident~        0.687  0.507   Pave   No_A~ Slightly~ Lvl         
##  4 One_Story_~ Resident~        1.04   0.128   Pave   No_A~ Regular   Lvl         
##  5 Two_Story_~ Resident~        0.480  0.454   Pave   No_A~ Slightly~ Lvl         
##  6 Two_Story_~ Resident~        0.598 -0.0156  Pave   No_A~ Slightly~ Lvl         
##  7 One_Story_~ Resident~       -0.496 -0.632   Pave   No_A~ Regular   Lvl         
##  8 Two_Story_~ Resident~        0.510 -0.0129  Pave   No_A~ Slightly~ Lvl         
##  9 One_Story_~ Resident~       -1.71  -0.259   Pave   No_A~ Slightly~ Lvl         
## 10 One_Story_~ Resident~        0.805  0.00851 Pave   No_A~ Regular   Lvl         
## # ... with 2,188 more rows, and 66 more variables: Utilities &lt;fct&gt;, Lot_Config &lt;fct&gt;,
## #   Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;, Condition_2 &lt;fct&gt;,
## #   Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, Overall_Cond &lt;fct&gt;, Year_Built &lt;dbl&gt;,
## #   Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;, Exterior_1st &lt;fct&gt;,
## #   Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;fct&gt;,
## #   Foundation &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, Bsmt_Exposure &lt;fct&gt;, BsmtFin_Type_1 &lt;fct&gt;,
## #   BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_Type_2 &lt;fct&gt;, BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;,
## #   Total_Bsmt_SF &lt;dbl&gt;, Heating &lt;fct&gt;, Heating_QC &lt;fct&gt;, Central_Air &lt;fct&gt;,
## #   Electrical &lt;fct&gt;, First_Flr_SF &lt;dbl&gt;, Second_Flr_SF &lt;dbl&gt;, Gr_Liv_Area &lt;dbl&gt;,
## #   Bsmt_Full_Bath &lt;dbl&gt;, Bsmt_Half_Bath &lt;dbl&gt;, Full_Bath &lt;dbl&gt;, Half_Bath &lt;dbl&gt;,
## #   Bedroom_AbvGr &lt;dbl&gt;, Kitchen_AbvGr &lt;dbl&gt;, TotRms_AbvGrd &lt;dbl&gt;, Functional &lt;fct&gt;,
## #   Fireplaces &lt;dbl&gt;, Garage_Type &lt;fct&gt;, Garage_Finish &lt;fct&gt;, Garage_Cars &lt;dbl&gt;,
## #   Garage_Area &lt;dbl&gt;, Garage_Cond &lt;fct&gt;, Paved_Drive &lt;fct&gt;, Wood_Deck_SF &lt;dbl&gt;,
## #   Open_Porch_SF &lt;dbl&gt;, Enclosed_Porch &lt;dbl&gt;, Three_season_porch &lt;dbl&gt;,
## #   Screen_Porch &lt;dbl&gt;, Pool_Area &lt;dbl&gt;, Pool_QC &lt;fct&gt;, Fence &lt;fct&gt;, Misc_Feature &lt;fct&gt;,
## #   Misc_Val &lt;dbl&gt;, Mo_Sold &lt;dbl&gt;, Year_Sold &lt;dbl&gt;, Sale_Type &lt;fct&gt;,
## #   Sale_Condition &lt;fct&gt;, Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, Sale_Price &lt;dbl&gt;
```

.font80[

&gt; "As steps are estimated by `prep()`, these operations are applied to the training set. 
Rather than running `bake()` to duplicate this processing, 
this function will return variables from the processed training set." &amp;mdash; ?recipes::juice

]

???

There are packages like embed, textrecipes, and themis that extend recipes with new steps.

---

class: exercise-blue, middle

## Your Turn 8

<div class="countdown" id="timer_5e99c674" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

Write a recipe for the ames data that:

1. adds a novel level to all factors
1. converts all factors to dummy variables
1. catches any zero variance variables
1. centers all of the predictors
1. scales all of the predictors
1. computes the first 3 principal components

Save the result as `pca_rec`.

---

exclude: true

roles

You can also give variables a "role" within a recipe and then select by roles.


```r
has_role(match = "privacy")
add_role(pca_rec, Fence, new_role = "privacy")
update_role(rec, Fence, new_role = "privacy", old_role = "yard")
remove_role(rec, Fence, old_role = "yard")
```

---

## A full workflow


```r
set.seed(123)
so_cv &lt;- vfold_cv(stackoverflow, v = 5)
so_rec &lt;- recipe(remote ~ ., data = stackoverflow) %&gt;% 
  step_dummy(all_nominal(), -all_outcomes()) %&gt;%
  step_corr(all_predictors(), threshold = 0.5)

tree_spec &lt;- decision_tree() %&gt;%
  set_engine("rpart") %&gt;%
  set_mode("classification")

so_wf &lt;- workflow() %&gt;% 
  add_model(tree_spec) %&gt;% 
* add_recipe(so_rec)

*fit_resamples(so_wf, # note: workflow object instead of model spec
              resamples = so_cv,
              metrics = metric_set(accuracy, sens, spec),
              control = control_resamples(save_pred = TRUE)) %&gt;%
  # collect_metrics() %&gt;% 
  collect_predictions() %&gt;%
  conf_mat(remote, .pred_class)
```

```
##             Truth
## Prediction   Remote Not remote
##   Remote        381        224
##   Not remote    194        351
```

---

You can tune models **and** recipes!


```r
pca_tuner &lt;- recipe(Sale_Price ~ ., data = ames) %&gt;%
    step_novel(all_nominal()) %&gt;%
    step_dummy(all_nominal()) %&gt;%
    step_zv(all_predictors()) %&gt;%
    step_center(all_predictors()) %&gt;%
    step_scale(all_predictors()) %&gt;%
*   step_pca(all_predictors(), num_comp = tune())
pca_twf &lt;- workflow() %&gt;% 
    add_recipe(pca_tuner) %&gt;% 
*   add_model(nearest_neighbor(neighbors = tune()) %&gt;%
                set_engine("kknn") %&gt;% set_mode("regression"))
*tg &lt;- expand_grid(num_comp = 2:10, neighbors = seq(1, 15, 4))
set.seed(100)
cv_folds &lt;- vfold_cv(ames, v = 5, strata = Sale_Price, breaks = 4)
set.seed(100)
pca_results &lt;- pca_twf %&gt;% 
    tune_grid(resamples = cv_folds, grid = tg)
pca_results %&gt;% show_best(metric = "rmse")
```

```
## # A tibble: 5 x 7
##   neighbors num_comp .metric .estimator   mean     n std_err
##       &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1         9        7 rmse    standard   31793.     5    968.
## 2        13        7 rmse    standard   31961.     5   1157.
## 3         9        8 rmse    standard   31963.     5   1099.
## 4         9        5 rmse    standard   32141.     5    951.
## 5        13        8 rmse    standard   32180.     5   1234.
```

---

.font70[


## Session info


```
##  setting  value                       
##  version  R version 3.6.3 (2020-02-29)
##  os       Windows 10 x64              
##  system   x86_64, mingw32             
##  ui       RTerm                       
##  language (EN)                        
##  collate  English_United States.1252  
##  ctype    English_United States.1252  
##  tz       Europe/Berlin               
##  date     2020-04-17
```

&lt;div style="font-size:80%;"&gt;

.pull-left[

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; package &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; version &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; date &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; source &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AmesHousing &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0.3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2017-12-17 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; broom &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5.5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-02-29 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; countdown &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.3.5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-03-20 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Github (gadenbuie/countdown@a544fa4) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; dials &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0.6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-04-03 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; dplyr &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.8.5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-03-07 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; forcats &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5.0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-03-01 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ggplot2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3.3.0.9000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-03-30 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Github (tidyverse/ggplot2@53815f8) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; infer &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5.1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2019-11-19 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; kknn &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.3.1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2016-03-26 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; parsnip &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.1.0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-04-09 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; patchwork &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.0.0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2019-12-01 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; purrr &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.3.3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2019-10-18 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; readr &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.3.1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2018-12-21 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

.pull-right[

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; package &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; version &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; date &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; source &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; recipes &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.1.10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-03-18 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; rpart &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.1.15 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2019-04-12 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; rpart.plot &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3.0.8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2019-08-22 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; rsample &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0.6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-03-31 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; scales &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.1.0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2019-11-18 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; stringr &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.4.0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2019-02-10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; tibble &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3.0.0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-03-30 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; tidymodels &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.1.0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-02-16 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; tidyr &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.0.2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-01-24 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; tidyverse &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.3.0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2019-11-21 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; tune &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.1.0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-04-02 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; workflows &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.1.1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-03-17 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; yardstick &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0.6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2020-03-17 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CRAN (R 3.6.3) &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

&lt;/div&gt;

]


---

class: last-slide, center, bottom

# Thank you! Questions?

&amp;nbsp;

.courtesy[&amp;#x1F4F7; Photo courtesy of Stefan Berger]

???

- tidymodels version 0.1 on CRAN = early development, no major release yet
- be aware that code from today may not work with a future version 
- currently, no comprehensive go-to resource for tidymodels
- still, it is very likely that tidymodels will at least have caret's functionality by 2021 and then work better together with the tidyverse, and that it will continuosly get better in the near future
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
